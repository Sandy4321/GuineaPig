Branching:
 master
 | mrs
 | | mrs_serve
 | | | mrs_serve_opt???

TODO for mrs_gp.py:
 -- mrs_gp - works with communicate, but only get about a 4x speedup, which maybe isn't that nice.
 possible that it would be better with more asynchronous behavior. 
 -- mrs_gp_v2a.py - clearer way of setting up arrays of pipes and such, but doesn't work
 -- mrs_gp_v3.py - my own more asynchronous version of _communicate, which seems to work,
 but the wrapper code doesn't exploit the asynchronous behavior
 -- mrs_gp_v2.py - trying to clean up mrs_gp_v3.py and make it more asynchronous - but it doesn't
 actually work anymore :-(
 -- mrs_gp_v4.py - cleaned up version which uses asynchronous communication.  
 seems ok for small problems but there is way too much overhead....only about 5% of the time
 is spent. 

ALSO: reduce tasks are all done sequentially, starting with the last one....for both v1 and v4???

 -- monitor mapper/reducer/shuffler min-max-avg
 -- server timeout, for security, in case I leave it running...?
 -- restrict opened files to a subdirectory - "./" + foo 
 -- add an 'abort task' method for server...? or maybe kill client thread?
 -- test with gpig, test on processes with errors, ....
 -- os.times() - can this show children's time/CPU pct so I can see actual load?
 -- optimize: can reducerQ write directly to reduce processes, rather than 
    buffering their inputs? can we write incrementally to/from mappers?
    would either help much? the mappers seem to start pretty quick,
    and the reducer will need to buffer up data in the sort....
 --- multiple map inputs, for joins --inputs dir1,dir2,...
 --- --port option
 --- reporting: subprocess io progress? pid status?  
 --- check if input file is a directory or not, and write to a file, not directory, if --numReduceTasks = 1

ROBUSTNESS/ERRORS/DOCS:
 - writeup smallvoc-tfidf.py as an example
 -- should I round-trip serialized outputs, to make sure they can be read back in?
 -- better docs for Augment?
 -- document it needs unix, python 2.7
 -- document lack of UDF's

FUNCTIONALITY - to add

to add:
 - MapPartitions(iteratorGenerator): iteratorGenerator(inputIterator) returns an iterator over mapped items
 - add StreamingMapReduce(view1, mapper='shell command', reducer='shell command', combiner='shell command', sideviews=..., shipping=[f1,..,fk]) 
 -- opt(1) 
 --- define class StreamingMapReduce(MapReduce): stores the commands
 --- make this a special case for the compiler - abstract compiler _coreCommand(self,step,gp) would just be mapper-command or reduce-command
 -- opt(2) 
 --- define class StreamingMapReduce(MapReduce): which knows how to doExternalMap and doExternalReduce, etc via subprocesses
 --- this could be a gpextras view

 - add user-defined Reuse(FILE) ? (why do I want this again? for make-style pipelines? is it really needed?)
 - gpextras, for debugging:
 -- PPrint?
 -- Wrap?
 -- WholeTextFiles?

 - cleanup
 -- standardize view.by argument
 -- clean up.gpmo and other tmp files? could do this via analysis at the AbstractMapReduceTask lst level
 -- log created views so you can continue with --reuse `cat foo.log|grep ^created|cut -f2`
 -- maybe add --config logging:warn,...

DOCS:
 - some longer examples for the tutorial (phirl-naive? tfidfs?)
 - document planner.ship, planner.setEvaluator

TODO - MAJOR

- a GPig.registerCompiler('key',factoryClass), for adding new targets other than hadoop?
 - compiler for marime.py map-reducer with ramdisks (note: diskutil erasevolume HFS+ 'RAMDisk' `hdiutil attach -nomount ram://10315776`,
   size is in 2048-byte blocks)

- multithreading ideas

  1. for CPU intensive steps, include multithreaded ReplaceEach and Flatten ops,
     which sets up a work queue, and for each row, adds a task, and
     removes any completed tasks.

  2. add another compiler, which generates marime/mime.py steps.

  3. Implement: marime mr -m MAP -r RED -i IN -o OUT -k K 
           and: marime putsplit -i IN -o OUT -k K
	        marime getmerge -i IN -o OUT
  where in and out are ramdisk directories

  map setup if there are J files in I:
    K queues, Qi for shard i - ie {k,v : H(k)%K == i}
    J processes Pj, each will run 'MAP < in/shard.j | ...' -- or could use threads (subprocesses would be more modular)
    J threads to read from Pj.stdout and route (k,v) to appropriate Qi
    K threads, each to process inputs from one queue, into a shardBuffer

  when all map processes are done:
    K subprocesses, Ri, to run '... | RED > out/shard.k' -- or could use threads (subprocesses more modular)
    K threads to print from the shardBuffer to Ri

 - DESCRIBE(...) - could be just a pretty_print?

 - ILLUSTRATE(view,[outputs]) - using definition of view, select the
   inputs from the innerviews that produce those outputs.  Then, do that
   recursively to get a test case. 



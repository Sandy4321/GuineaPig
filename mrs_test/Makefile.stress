#comments: with full_test.txt, about 100M
#  disk and ram times are comparable
#  ram is fastest w/o parallelism (K=1), then it's about 2x speed of disk
# with full_train.txt, about 1G
#  ram is much faster
#  not much speedup w parallelism (K=1 is best)


RAM=/mnt/ramdisk/mrs/
K=60
BIGFILE=/afs/cs.cmu.edu/user/wcohen/bigML/RCV1/RCV1.full_test.txt
#BIGFILE=/afs/cs.cmu.edu/user/wcohen/bigML/RCV1/RCV1.full_train.txt
DIR=$(RAM)
NBCOUNT='python streamNaiveBayesLearner.py --streamTrain 100'
#NBCOUNT='python streamNaiveBayesLearner.py --train'
SUM='python sum-events.py'

MRS=pypy ../mrs_gp.py

simple-disk-times:
	echo == disk stream - about 3 sec for 100M, 2:03 for 1G
	time cat $(BIGFILE) | python fixrcv.py > bigfile1/test.txt
	echo == sharding disk-disk - about 10-11 sec for 100M with K=10, 1:18 for 1G with K=5
	time $(MRS) --input bigfile1 --output bigfile1-sharded --mapper cat --reducer cat --numReduceTasks $(K)
	echo == copy sharded disk-disk - about 8-10 sec	for 100M with K=10, 1:16 for 1G with K=5
	time $(MRS) --input bigfile1-sharded --output bigfile1-copy --mapper cat --reducer cat --numReduceTasks $(K)

simple-ram-times:
	echo == disk to ram stream - about 3 sec for 100M, 3:19 for 1G with K=5
	time cat /afs/cs.cmu.edu/user/wcohen/bigML/RCV1/RCV1.full_test.txt | python fixrcv.py > $(RAM)/bigfile1/test.txt
	echo == sharding ram-ram - about 10-11 sec for 100M , 0:08 for 1G with K=5
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks $(K)
	echo == copy sharded ram-ram - about 8-10 sec for 100M, 0:07 for 1G with K=5	
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks $(K)        
	echo == resharded ram-ram - about 8.7 sec for 100M with k=30
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks `expr $(K) / 2`

# for train data - 1G 
# k	shard	copy
# 1	5.7	5.4   
# 3	7.7	7.3	
# 5	7.9	7.5
# 10	9.0	8.2
# 20	9.7	8.8	

simple-ram-times-sweep-k:
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks 1
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks 1
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks 3
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks 3
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks 5
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks 5
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks 10
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks 10
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks 20
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks 20

# for test data - 100M --train
#      version 1 	  version 2  	    pypy v2
# k	shard	learn	shard	  learn	    shard	learn
# 1	5.7	 3:44	2.8	  3:05	    1:27	3:46
# 3	6.7	 8:12	3.0	  3:38	    1:28	7:34
# 10	8.5	14:02	
# 20	9.0	20:40	3.5	  7:10	    1:30	26:32

# for test data - 100M --streamTrain 100 pypy
# k	shard	learn	%learnCPU
# 1	1:31	  26:49		102
# 3	1:29	1:01:35		250
# 10
# 20

learn-ram-times:
	echo sharding
	$(MRS) --serve
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks $(K)
	echo learning
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/events --numReduceTasks $(K) --mapper $(NBCOUNT) --reducer $(SUM)
	$(MRS) --shutdown

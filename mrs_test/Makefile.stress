#comments: with full_test.txt, about 100M
#  disk and ram times are comparable
#  ram is fastest w/o parallelism (K=1), then it's about 2x speed of disk
# with full_train.txt, about 1G
#  ram is much faster
#  not much speedup w parallelism (K=1 is best)


RAM=/mnt/ramdisk/mrs/
K=30
BIGFILE=/afs/cs.cmu.edu/user/wcohen/bigML/RCV1/RCV1.full_test.txt
#BIGFILE=/afs/cs.cmu.edu/user/wcohen/bigML/RCV1/RCV1.full_train.txt
DIR=$(RAM)
NBCOUNT='python streamNaiveBayesLearner.py --train'
SUM='python sum-events.py'

MRS=python ../mrs_gp.py

make simple-disk-times:
	echo == disk stream - about 3 sec for 100M, 2:03 for 1G
	time cat $(BIGFILE) | python fixrcv.py > bigfile1/test.txt
	echo == sharding disk-disk - about 10-11 sec for 100M with K=10, 1:18 for 1G with K=5
	time $(MRS) --input bigfile1 --output bigfile1-sharded --mapper cat --reducer cat --numReduceTasks $(K)
	echo == copy sharded disk-disk - about 8-10 sec	for 100M with K=10, 1:16 for 1G with K=5
	time $(MRS) --input bigfile1-sharded --output bigfile1-copy --mapper cat --reducer cat --numReduceTasks $(K)

make simple-ram-times:
	echo == disk to ram stream - about 3 sec for 100M, 3:19 for 1G with K=5
	time cat /afs/cs.cmu.edu/user/wcohen/bigML/RCV1/RCV1.full_test.txt | python fixrcv.py > $(RAM)/bigfile1/test.txt
	echo == sharding ram-ram - about 10-11 sec for 100M , 0:08 for 1G with K=5
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks $(K)
	echo == copy sharded ram-ram - about 8-10 sec for 100M, 0:07 for 1G with K=5	
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks $(K)

# for train data - 1G 
# k	shard	copy
# 1	5.7	5.4
# 3	7.7	7.3	
# 5	7.9	7.5
# 10	9.0	8.2
# 20	9.7	8.8

make simple-ram-times-sweep-k:
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks 1
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks 1
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks 3
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks 3
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks 5
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks 5
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks 10
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks 10
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks 20
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/bigfile1-copy --mapper cat --reducer cat --numReduceTasks 20

# for test data - 100M
# k	shard	learn
# 1	
# 3	
# 5	7.9
# 10	
# 20	

make learn-ram-times:
	echo sharding
	time $(MRS) --input $(RAM)/bigfile1 --output $(RAM)/bigfile1-sharded --mapper cat --reducer cat --numReduceTasks $(K)
	echo learning
	time $(MRS) --input $(RAM)/bigfile1-sharded --output $(RAM)/events --numReduceTasks $(K) --mapper $(NBCOUNT) --reducer $(SUM)
